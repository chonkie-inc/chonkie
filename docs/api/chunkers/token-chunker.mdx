---
title: "Token Chunker"
api: "POST /v1/chunk/token"
description: "Split text into fixed-size token chunks with configurable overlap"
---

The Token Chunker splits text into chunks based on token count, ensuring each chunk stays within specified token limits.

## Examples

### Text Input

<CodeGroup>

```python Python
from chonkie.cloud import TokenChunker

chunker = TokenChunker(
    tokenizer="gpt2",
    chunk_size=512,
    chunk_overlap=128
)

text = "Your text here..."
chunks = chunker.chunk(text)
```

```javascript JavaScript
import { TokenChunker } from '@chonkiejs/cloud';

const chunker = new TokenChunker({
  tokenizer: 'gpt2',
  chunkSize: 512,
  chunkOverlap: 128
});

const text = "Your text here...";
const chunks = await chunker.chunk({ text });
```

```bash cURL
curl -X POST https://api.chonkie.ai/v1/chunk/token \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Your text here...",
    "tokenizer": "gpt2",
    "chunk_size": 512,
    "chunk_overlap": 128
  }'
```

</CodeGroup>

### File Input

<CodeGroup>

```python Python
from chonkie.cloud import TokenChunker

chunker = TokenChunker(
    tokenizer="gpt2",
    chunk_size=512,
    chunk_overlap=128
)

# Chunk from file
with open("document.txt", "rb") as f:
    chunks = chunker.chunk(file=f)
```

```javascript JavaScript
import { TokenChunker } from '@chonkiejs/cloud';

const chunker = new TokenChunker({
  tokenizer: 'gpt2',
  chunkSize: 512,
  chunkOverlap: 128
});

// Chunk from file path (Node.js)
const chunks = await chunker.chunk({ filepath: './document.txt' });
```

```bash cURL
# Step 1: Upload the file
curl -X POST https://api.chonkie.ai/v1/files \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -F "file=@document.txt"

# Returns: {"name": "document.txt", "size": "1024"}

# Step 2: Chunk the uploaded file
curl -X POST https://api.chonkie.ai/v1/chunk/token \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "file": {
      "type": "document",
      "content": "document.txt"
    },
    "tokenizer": "gpt2",
    "chunk_size": 512,
    "chunk_overlap": 128
  }'
```

</CodeGroup>

## Request

#### Parameters

<ParamField path="text" type="string | string[]">
  The text to chunk. Can be a single string or an array of strings for batch processing. Either `text` or `file` is required.
</ParamField>

<ParamField path="file" type="file">
  File to chunk. Use multipart/form-data encoding. Either `text` or `file` is required.
</ParamField>

<ParamField path="tokenizer" type="string" default="gpt2">
  Tokenizer to use for counting tokens. Options: "gpt2", "character", "word", or any Hugging Face tokenizer.
</ParamField>

<ParamField path="chunk_size" type="integer" default="512">
  Maximum number of tokens per chunk.
</ParamField>

<ParamField path="chunk_overlap" type="integer" default="0">
  Number of tokens to overlap between consecutive chunks.
</ParamField>

## Response

#### Returns

Array of `Chunk` objects, each containing:

<ResponseField name="text" type="string">
  The chunk text content.
</ResponseField>

<ResponseField name="start_index" type="integer">
  Starting character position in the original text.
</ResponseField>

<ResponseField name="end_index" type="integer">
  Ending character position in the original text.
</ResponseField>

<ResponseField name="token_count" type="integer">
  Number of tokens in the chunk.
</ResponseField>

