---
title: "Slumber Chunker"
api: "POST https://api.chonkie.ai/v1/chunk/slumber"
description: "Efficient chunking with sliding window for long documents"
---

The Slumber Chunker uses a sliding window approach with efficient processing for chunking long documents while maintaining context.

## Examples

### Text Input

<CodeGroup>

```python Python
from chonkie.cloud import SlumberChunker

chunker = SlumberChunker(
    chunk_size=512,
    recipe="markdown"
)

text = "Your text here..."
chunks = chunker.chunk(text)
```

```javascript JavaScript
import { SlumberChunker } from "@chonkiejs/cloud";

const chunker = new SlumberChunker({
  chunkSize: 512,
  recipe: "markdown"
});

const text = "Your text here...";
const chunks = await chunker.chunk({ text });
```

```bash cURL
curl -X POST https://api.chonkie.ai/v1/chunk/slumber \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Your text here...",
    "chunk_size": 512,
    "recipe": "markdown"
  }'
```

</CodeGroup>

### File Input

<CodeGroup>

```python Python
from chonkie.cloud import SlumberChunker

chunker = SlumberChunker(
    chunk_size=512,
    recipe="markdown"
)

# Chunk from file
with open("document.txt", "rb") as f:
    chunks = chunker.chunk(file=f)
```

```javascript JavaScript
import { SlumberChunker } from "@chonkiejs/cloud";

const chunker = new SlumberChunker({
  chunkSize: 512,
  recipe: "markdown"
});

// Chunk from file
const file = document.querySelector('input[type="file"]').files[0];
const chunks = await chunker.chunk({ file });
```

```bash cURL
curl -X POST https://api.chonkie.ai/v1/chunk/slumber \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -F "file=@document.txt" \
  -F "chunk_size=512" \
  -F "recipe=markdown"
```

</CodeGroup>

## Request

#### Parameters

<ParamField path="text" type="string | string[]">
  The text to chunk. Can be a single string or an array of strings for batch processing. Either `text` or `file` is required.
</ParamField>

<ParamField path="file" type="file">
  File to chunk. Use multipart/form-data encoding. Either `text` or `file` is required.
</ParamField>

<ParamField path="tokenizer" type="string" default="gpt2">
  Tokenizer to use for counting tokens.
</ParamField>

<ParamField path="chunk_size" type="integer" default="512">
  Maximum number of tokens per chunk.
</ParamField>

<ParamField path="recipe" type="string" default="default">
  Recursive rules recipe to follow. See all recipes on our [Hugging Face
  repo](https://huggingface.co/datasets/chonkie-ai/recipes)
</ParamField>

<ParamField path="lang" type="string" default="en">
  Language of the document to chunk
</ParamField>

<ParamField path="candidate_size" type="integer" default="128">
  The size of the candidate splits that the chunker will consider.
</ParamField>

<ParamField path="min_characters_per_chunk" type="integer" default="24">
  Minimum number of characters per chunk
</ParamField>

## Response

#### Returns

Array of `Chunk` objects, each containing:

<ResponseField name="text" type="string">
  The chunk text content.
</ResponseField>

<ResponseField name="start_index" type="integer">
  Starting character position in the original text.
</ResponseField>

<ResponseField name="end_index" type="integer">
  Ending character position in the original text.
</ResponseField>

<ResponseField name="token_count" type="integer">
  Number of tokens in the chunk.
</ResponseField>
