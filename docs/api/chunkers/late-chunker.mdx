---
title: "Late Chunker"
api: "POST https://api.chonkie.ai/v1/chunk/late"
description: "Late chunking with contextual embeddings for each token"
---

The Late Chunker generates contextual embeddings for each token before chunking, enabling more semantically aware chunk boundaries.

## Examples

### Text Input

<CodeGroup>

```python Python
from chonkie.cloud import LateChunker

chunker = LateChunker(
    embedding_model="minishlab/potion-base-8M",
    chunk_size=512
)

text = "Your text here..."
chunks = chunker.chunk(text)
```

```javascript JavaScript
import { LateChunker } from "@chonkiejs/cloud";

const chunker = new LateChunker({
  embeddingModel: "minishlab/potion-base-8M",
  chunkSize: 512,
});

const text = "Your text here...";
const chunks = await chunker.chunk({ text });
```

```bash cURL
curl -X POST https://api.chonkie.ai/v1/chunk/late \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Your text here...",
    "embedding_model": "minishlab/potion-base-8M",
    "chunk_size": 512
  }'
```

</CodeGroup>

### File Input

<CodeGroup>

```python Python
from chonkie.cloud import LateChunker

chunker = LateChunker(
    embedding_model="minishlab/potion-base-8M",
    chunk_size=512
)

# Chunk from file
with open("document.txt", "rb") as f:
    chunks = chunker.chunk(file=f)
```

```javascript JavaScript
import { LateChunker } from "@chonkiejs/cloud";

const chunker = new LateChunker({
  embeddingModel: "minishlab/potion-base-8M",
  chunkSize: 512,
});

// Chunk from file
const file = document.querySelector('input[type="file"]').files[0];
const chunks = await chunker.chunk({ file });
```

```bash cURL
curl -X POST https://api.chonkie.ai/v1/chunk/late \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -F "file=@document.txt" \
  -F "embedding_model=minishlab/potion-base-8M" \
  -F "chunk_size=512"
```

</CodeGroup>

## Request

#### Parameters

<ParamField path="text" type="string | string[]">
  The text to chunk. Can be a single string or an array of strings for batch processing. Either `text` or `file` is required.
</ParamField>

<ParamField path="file" type="file">
  File to chunk. Use multipart/form-data encoding. Either `text` or `file` is required.
</ParamField>

<ParamField
  path="embedding_model"
  type="string"
  default="minishlab/potion-base-8M"
>
  The embedding model to use.
  <Note>
    {" "}
    Chosen embedding model must return token level embeddings. Request will fail
    otherwise{" "}
  </Note>
</ParamField>

<ParamField path="chunk_size" type="integer" default="512">
  Maximum number of tokens per chunk.
</ParamField>

<ParamField path="recipe" type="string" default="default">
  Recursive rules recipe to follow. See all recipes on our [Hugging Face
  repo](https://huggingface.co/datasets/chonkie-ai/recipes)
</ParamField>

<ParamField path="lang" type="string" default="en">
  Language of the document to chunk
</ParamField>

## Response

#### Returns

Array of `Chunk` objects, each containing:

<ResponseField name="text" type="string">
  The chunk text content.
</ResponseField>

<ResponseField name="start_index" type="integer">
  Starting character position in the original text.
</ResponseField>

<ResponseField name="end_index" type="integer">
  Ending character position in the original text.
</ResponseField>

<ResponseField name="token_count" type="integer">
  Number of tokens in the chunk.
</ResponseField>
